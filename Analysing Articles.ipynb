{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a489c04e",
   "metadata": {},
   "source": [
    "# Project 3 Team 5\n",
    "## Date: 17 January 2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b884053",
   "metadata": {},
   "source": [
    "1. Project Overview\n",
    "2. Data Load and preprocessing\n",
    "3. Model training\n",
    "4. Evaluation\n",
    "5. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef8bae",
   "metadata": {},
   "source": [
    "### 1. Project Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028adbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3861b463",
   "metadata": {},
   "source": [
    "### 2. Data load and preprocessing \n",
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64014f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61dde8",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241ffc87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>The central bank has also asked chairpersons a...</td>\n",
       "      <td>The Reserve Bank of India (RBI) has changed th...</td>\n",
       "      <td>https://indianexpress.com/article/business/ban...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>NDTV's consolidated revenue from operations wa...</td>\n",
       "      <td>Broadcaster New Delhi Television Ltd on Monday...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>The initial share sale will be open for public...</td>\n",
       "      <td>Homegrown server maker Netweb Technologies Ind...</td>\n",
       "      <td>https://indianexpress.com/article/business/mar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>The current account deficit (CAD) was 3.8 per ...</td>\n",
       "      <td>India’s current account deficit declined sharp...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>The prices shot up reflecting the overall high...</td>\n",
       "      <td>States have been forced to pay through their n...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  RBI revises definition of politically-exposed ...   \n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...   \n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...   \n",
       "3  India’s current account deficit declines sharp...   \n",
       "4  States borrowing cost soars to 7.68%, highest ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The central bank has also asked chairpersons a...   \n",
       "1  NDTV's consolidated revenue from operations wa...   \n",
       "2  The initial share sale will be open for public...   \n",
       "3  The current account deficit (CAD) was 3.8 per ...   \n",
       "4  The prices shot up reflecting the overall high...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The Reserve Bank of India (RBI) has changed th...   \n",
       "1  Broadcaster New Delhi Television Ltd on Monday...   \n",
       "2  Homegrown server maker Netweb Technologies Ind...   \n",
       "3  India’s current account deficit declined sharp...   \n",
       "4  States have been forced to pay through their n...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  https://indianexpress.com/article/business/ban...  business  \n",
       "1  https://indianexpress.com/article/business/com...  business  \n",
       "2  https://indianexpress.com/article/business/mar...  business  \n",
       "3  https://indianexpress.com/article/business/eco...  business  \n",
       "4  https://indianexpress.com/article/business/eco...  business  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the dataset:the train dataset \n",
    "train_df = pd.read_csv('C:/Users/lebog/Documents/Explore AI academy/7. NLP and classification/train.csv')\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90f6f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NLC India wins contract for power supply to Ra...</td>\n",
       "      <td>State-owned firm NLC India Ltd (NLCIL) on Mond...</td>\n",
       "      <td>State-owned firm NLC India Ltd (NLCIL) on Mond...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBI Clerk prelims exams dates announced; admit...</td>\n",
       "      <td>SBI Clerk Prelims Exam: The SBI Clerk prelims ...</td>\n",
       "      <td>SBI Clerk Prelims Exam: The State Bank of Indi...</td>\n",
       "      <td>https://indianexpress.com/article/education/sb...</td>\n",
       "      <td>education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Golden Globes: Michelle Yeoh, Will Ferrell, An...</td>\n",
       "      <td>Barbie is the top nominee this year, followed ...</td>\n",
       "      <td>Michelle Yeoh, Will Ferrell, Angela Bassett an...</td>\n",
       "      <td>https://indianexpress.com/article/entertainmen...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OnePlus Nord 3 at Rs 27,999 as part of new pri...</td>\n",
       "      <td>New deal makes the OnePlus Nord 3 an easy purc...</td>\n",
       "      <td>In our review of the OnePlus Nord 3 5G, we pra...</td>\n",
       "      <td>https://indianexpress.com/article/technology/t...</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adani family’s partners used ‘opaque’ funds to...</td>\n",
       "      <td>Citing review of files from multiple tax haven...</td>\n",
       "      <td>Millions of dollars were invested in some publ...</td>\n",
       "      <td>https://indianexpress.com/article/business/ada...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  NLC India wins contract for power supply to Ra...   \n",
       "1  SBI Clerk prelims exams dates announced; admit...   \n",
       "2  Golden Globes: Michelle Yeoh, Will Ferrell, An...   \n",
       "3  OnePlus Nord 3 at Rs 27,999 as part of new pri...   \n",
       "4  Adani family’s partners used ‘opaque’ funds to...   \n",
       "\n",
       "                                         description  \\\n",
       "0  State-owned firm NLC India Ltd (NLCIL) on Mond...   \n",
       "1  SBI Clerk Prelims Exam: The SBI Clerk prelims ...   \n",
       "2  Barbie is the top nominee this year, followed ...   \n",
       "3  New deal makes the OnePlus Nord 3 an easy purc...   \n",
       "4  Citing review of files from multiple tax haven...   \n",
       "\n",
       "                                             content  \\\n",
       "0  State-owned firm NLC India Ltd (NLCIL) on Mond...   \n",
       "1  SBI Clerk Prelims Exam: The State Bank of Indi...   \n",
       "2  Michelle Yeoh, Will Ferrell, Angela Bassett an...   \n",
       "3  In our review of the OnePlus Nord 3 5G, we pra...   \n",
       "4  Millions of dollars were invested in some publ...   \n",
       "\n",
       "                                                 url       category  \n",
       "0  https://indianexpress.com/article/business/com...       business  \n",
       "1  https://indianexpress.com/article/education/sb...      education  \n",
       "2  https://indianexpress.com/article/entertainmen...  entertainment  \n",
       "3  https://indianexpress.com/article/technology/t...     technology  \n",
       "4  https://indianexpress.com/article/business/ada...       business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the dataset: the test \n",
    "test_df = pd.read_csv('C:/Users/lebog/Documents/Explore AI academy/7. NLP and classification/test.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d829d",
   "metadata": {},
   "source": [
    "#### 2.1. Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e821ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions for the Train set are: (5520, 5)\n",
      "The dimensions for the test set are: (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"The dimensions for the Train set are:\", train_df.shape)\n",
    "print(\"The dimensions for the test set are:\",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7ee9339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5520 entries, 0 to 5519\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   headlines    5520 non-null   object\n",
      " 1   description  5520 non-null   object\n",
      " 2   content      5520 non-null   object\n",
      " 3   url          5520 non-null   object\n",
      " 4   category     5520 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 215.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   headlines    2000 non-null   object\n",
      " 1   description  2000 non-null   object\n",
      " 2   content      2000 non-null   object\n",
      " 3   url          2000 non-null   object\n",
      " 4   category     2000 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 78.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.info(), test_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6273263d",
   "metadata": {},
   "source": [
    "There are no missing values in both the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfc2cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Train Null Count  Test Null Count\n",
      "headlines                   0                0\n",
      "description                 0                0\n",
      "content                     0                0\n",
      "url                         0                0\n",
      "category                    0                0\n"
     ]
    }
   ],
   "source": [
    "null_counts = pd.DataFrame({\n",
    "    \"Train Null Count\": train_df.isnull().sum(),\n",
    "    \"Test Null Count\": test_df.isnull().sum()\n",
    "})\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a820a583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df.duplicated().sum(), test_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6cfce",
   "metadata": {},
   "source": [
    "Number of duplicate for train set rows: 0\n",
    "\n",
    "Number of duplicate for test set rows: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf638e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Train Dataset  Test Dataset\n",
      "0        Number of Rows           5520          2000\n",
      "1     Number of Columns              5             5\n",
      "2  Total Missing Values              0             0\n",
      "3        Duplicate Rows              0             0\n",
      "4     Numerical Columns              0             0\n",
      "5   Categorical Columns              5             5\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(df):\n",
    "    return {\n",
    "        \"Number of Rows\": df.shape[0],\n",
    "        \"Number of Columns\": df.shape[1],\n",
    "        \"Total Missing Values\": df.isnull().sum().sum(),\n",
    "        \"Duplicate Rows\": df.duplicated().sum(),\n",
    "        \"Numerical Columns\": df.select_dtypes(include='number').shape[1],\n",
    "        \"Categorical Columns\": df.select_dtypes(include='object').shape[1],\n",
    "    }\n",
    "\n",
    "train_inspection = inspect_dataset(train_df)\n",
    "test_inspection = inspect_dataset(test_df)\n",
    "\n",
    "inspection_results = pd.DataFrame({\n",
    "    \"Metric\": train_inspection.keys(),\n",
    "    \"Train Dataset\": train_inspection.values(),\n",
    "    \"Test Dataset\": test_inspection.values(),\n",
    "})\n",
    "\n",
    "print(inspection_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d719b",
   "metadata": {},
   "source": [
    "#### 2.2. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5679e",
   "metadata": {},
   "source": [
    "2.2.1. Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1aadbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "                                           headlines  \\\n",
      "0  RBI revises definition of politicallyexposed p...   \n",
      "1  NDTV Q net profit falls  to Rs  crore impacted...   \n",
      "2  Akasa Air well capitalised can grow much faste...   \n",
      "3  Indias current account deficit declines sharpl...   \n",
      "4  States borrowing cost soars to  highest so far...   \n",
      "\n",
      "                                         description  \\\n",
      "0  The central bank has also asked chairpersons a...   \n",
      "1  NDTVs consolidated revenue from operations was...   \n",
      "2  The initial share sale will be open for public...   \n",
      "3  The current account deficit CAD was  per cent ...   \n",
      "4  The prices shot up reflecting the overall high...   \n",
      "\n",
      "                                             content  \\\n",
      "0  The Reserve Bank of India RBI has changed the ...   \n",
      "1  Broadcaster New Delhi Television Ltd on Monday...   \n",
      "2  Homegrown server maker Netweb Technologies Ind...   \n",
      "3  Indias current account deficit declined sharpl...   \n",
      "4  States have been forced to pay through their n...   \n",
      "\n",
      "                                                 url  category  \n",
      "0  https://indianexpress.com/article/business/ban...  business  \n",
      "1  https://indianexpress.com/article/business/com...  business  \n",
      "2  https://indianexpress.com/article/business/mar...  business  \n",
      "3  https://indianexpress.com/article/business/eco...  business  \n",
      "4  https://indianexpress.com/article/business/eco...  business  \n",
      "\n",
      "Test DataFrame:\n",
      "                                           headlines  \\\n",
      "0  NLC India wins contract for power supply to Ra...   \n",
      "1  SBI Clerk prelims exams dates announced admit ...   \n",
      "2  Golden Globes Michelle Yeoh Will Ferrell Angel...   \n",
      "3  OnePlus Nord  at Rs  as part of new price cut ...   \n",
      "4  Adani familys partners used opaque funds to in...   \n",
      "\n",
      "                                         description  \\\n",
      "0  State-owned firm NLC India Ltd (NLCIL) on Mond...   \n",
      "1  SBI Clerk Prelims Exam: The SBI Clerk prelims ...   \n",
      "2  Barbie is the top nominee this year, followed ...   \n",
      "3  New deal makes the OnePlus Nord 3 an easy purc...   \n",
      "4  Citing review of files from multiple tax haven...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Stateowned firm NLC India Ltd NLCIL on Monday ...   \n",
      "1  SBI Clerk Prelims Exam The State Bank of India...   \n",
      "2  Michelle Yeoh Will Ferrell Angela Bassett and ...   \n",
      "3  In our review of the OnePlus Nord  G we praise...   \n",
      "4  Millions of dollars were invested in some publ...   \n",
      "\n",
      "                                                 url       category  \n",
      "0  https://indianexpress.com/article/business/com...       business  \n",
      "1  https://indianexpress.com/article/education/sb...      education  \n",
      "2  https://indianexpress.com/article/entertainmen...  entertainment  \n",
      "3  https://indianexpress.com/article/technology/t...     technology  \n",
      "4  https://indianexpress.com/article/business/ada...       business  \n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Apply the function to both columns\n",
    "train_df['headlines'] = train_df['headlines'].apply(remove_punctuation)\n",
    "train_df['description'] = train_df['description'].apply(remove_punctuation)\n",
    "train_df['content'] = train_df['content'].apply(remove_punctuation)\n",
    "\n",
    "test_df['headlines'] = test_df['headlines'].apply(remove_punctuation)\n",
    "test_df['content'] = test_df['content'].apply(remove_punctuation)\n",
    "test_df['content'] = test_df['content'].apply(remove_punctuation)\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfde9d",
   "metadata": {},
   "source": [
    "2.2.2. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c59ae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           headlines  \\\n",
      "0  rbi revises definition of politicallyexposed p...   \n",
      "1  ndtv q net profit falls  to rs  crore impacted...   \n",
      "2  akasa air well capitalised can grow much faste...   \n",
      "3  indias current account deficit declines sharpl...   \n",
      "4  states borrowing cost soars to  highest so far...   \n",
      "\n",
      "                                         description  \\\n",
      "0  the central bank has also asked chairpersons a...   \n",
      "1  ndtvs consolidated revenue from operations was...   \n",
      "2  the initial share sale will be open for public...   \n",
      "3  the current account deficit cad was  per cent ...   \n",
      "4  the prices shot up reflecting the overall high...   \n",
      "\n",
      "                                             content  \\\n",
      "0  the reserve bank of india rbi has changed the ...   \n",
      "1  broadcaster new delhi television ltd on monday...   \n",
      "2  homegrown server maker netweb technologies ind...   \n",
      "3  indias current account deficit declined sharpl...   \n",
      "4  states have been forced to pay through their n...   \n",
      "\n",
      "                                                 url  category  \n",
      "0  https://indianexpress.com/article/business/ban...  business  \n",
      "1  https://indianexpress.com/article/business/com...  business  \n",
      "2  https://indianexpress.com/article/business/mar...  business  \n",
      "3  https://indianexpress.com/article/business/eco...  business  \n",
      "4  https://indianexpress.com/article/business/eco...  business  \n",
      "                                           headlines  \\\n",
      "0  nlc india wins contract for power supply to ra...   \n",
      "1  sbi clerk prelims exams dates announced admit ...   \n",
      "2  golden globes michelle yeoh will ferrell angel...   \n",
      "3  oneplus nord  at rs  as part of new price cut ...   \n",
      "4  adani familys partners used opaque funds to in...   \n",
      "\n",
      "                                         description  \\\n",
      "0  state-owned firm nlc india ltd (nlcil) on mond...   \n",
      "1  sbi clerk prelims exam: the sbi clerk prelims ...   \n",
      "2  barbie is the top nominee this year, followed ...   \n",
      "3  new deal makes the oneplus nord 3 an easy purc...   \n",
      "4  citing review of files from multiple tax haven...   \n",
      "\n",
      "                                             content  \\\n",
      "0  stateowned firm nlc india ltd nlcil on monday ...   \n",
      "1  sbi clerk prelims exam the state bank of india...   \n",
      "2  michelle yeoh will ferrell angela bassett and ...   \n",
      "3  in our review of the oneplus nord  g we praise...   \n",
      "4  millions of dollars were invested in some publ...   \n",
      "\n",
      "                                                 url       category  \n",
      "0  https://indianexpress.com/article/business/com...       business  \n",
      "1  https://indianexpress.com/article/education/sb...      education  \n",
      "2  https://indianexpress.com/article/entertainmen...  entertainment  \n",
      "3  https://indianexpress.com/article/technology/t...     technology  \n",
      "4  https://indianexpress.com/article/business/ada...       business  \n"
     ]
    }
   ],
   "source": [
    "# Convert specified columns to lowercase\n",
    "columns_to_lowercase = ['headlines', 'description', 'content']\n",
    "\n",
    "for col in columns_to_lowercase:\n",
    "    train_df[col] = train_df[col].str.lower()\n",
    "    test_df[col] = test_df[col].str.lower()\n",
    "\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2911c0",
   "metadata": {},
   "source": [
    "2.2.3. Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c444d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           headlines  \\\n",
      "0  [rbi, revises, definition, of, politicallyexpo...   \n",
      "1  [ndtv, q, net, profit, falls, to, rs, crore, i...   \n",
      "2  [akasa, air, well, capitalised, can, grow, muc...   \n",
      "3  [indias, current, account, deficit, declines, ...   \n",
      "4  [states, borrowing, cost, soars, to, highest, ...   \n",
      "\n",
      "                                         description  \\\n",
      "0  [the, central, bank, has, also, asked, chairpe...   \n",
      "1  [ndtvs, consolidated, revenue, from, operation...   \n",
      "2  [the, initial, share, sale, will, be, open, fo...   \n",
      "3  [the, current, account, deficit, cad, was, per...   \n",
      "4  [the, prices, shot, up, reflecting, the, overa...   \n",
      "\n",
      "                                             content  \\\n",
      "0  [the, reserve, bank, of, india, rbi, has, chan...   \n",
      "1  [broadcaster, new, delhi, television, ltd, on,...   \n",
      "2  [homegrown, server, maker, netweb, technologie...   \n",
      "3  [indias, current, account, deficit, declined, ...   \n",
      "4  [states, have, been, forced, to, pay, through,...   \n",
      "\n",
      "                                                 url  category  \n",
      "0  https://indianexpress.com/article/business/ban...  business  \n",
      "1  https://indianexpress.com/article/business/com...  business  \n",
      "2  https://indianexpress.com/article/business/mar...  business  \n",
      "3  https://indianexpress.com/article/business/eco...  business  \n",
      "4  https://indianexpress.com/article/business/eco...  business  \n",
      "                                           headlines  \\\n",
      "0  [nlc, india, wins, contract, for, power, suppl...   \n",
      "1  [sbi, clerk, prelims, exams, dates, announced,...   \n",
      "2  [golden, globes, michelle, yeoh, will, ferrell...   \n",
      "3  [oneplus, nord, at, rs, as, part, of, new, pri...   \n",
      "4  [adani, familys, partners, used, opaque, funds...   \n",
      "\n",
      "                                         description  \\\n",
      "0  [state-owned, firm, nlc, india, ltd, (, nlcil,...   \n",
      "1  [sbi, clerk, prelims, exam, :, the, sbi, clerk...   \n",
      "2  [barbie, is, the, top, nominee, this, year, ,,...   \n",
      "3  [new, deal, makes, the, oneplus, nord, 3, an, ...   \n",
      "4  [citing, review, of, files, from, multiple, ta...   \n",
      "\n",
      "                                             content  \\\n",
      "0  [stateowned, firm, nlc, india, ltd, nlcil, on,...   \n",
      "1  [sbi, clerk, prelims, exam, the, state, bank, ...   \n",
      "2  [michelle, yeoh, will, ferrell, angela, basset...   \n",
      "3  [in, our, review, of, the, oneplus, nord, g, w...   \n",
      "4  [millions, of, dollars, were, invested, in, so...   \n",
      "\n",
      "                                                 url       category  \n",
      "0  https://indianexpress.com/article/business/com...       business  \n",
      "1  https://indianexpress.com/article/education/sb...      education  \n",
      "2  https://indianexpress.com/article/entertainmen...  entertainment  \n",
      "3  https://indianexpress.com/article/technology/t...     technology  \n",
      "4  https://indianexpress.com/article/business/ada...       business  \n"
     ]
    }
   ],
   "source": [
    "columns_to_tokenize = ['headlines', 'description', 'content']\n",
    "\n",
    "for col in columns_to_tokenize:\n",
    "    train_df[col] = train_df[col].apply(word_tokenize)\n",
    "    test_df[col] = test_df[col].apply(word_tokenize)\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff2ab4",
   "metadata": {},
   "source": [
    "2.2.4. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ef5b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lebog\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Apply tokenization and stopword removal to both train and test datasets\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns_to_process:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Tokenize the text (only once)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     train_df[col] \u001b[38;5;241m=\u001b[39m train_df[col]\u001b[38;5;241m.\u001b[39mapply(word_tokenize)\n\u001b[0;32m     24\u001b[0m     test_df[col] \u001b[38;5;241m=\u001b[39m test_df[col]\u001b[38;5;241m.\u001b[39mapply(word_tokenize)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Remove stopwords (after tokenization)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_vars\u001b[38;5;241m.\u001b[39mperiod_context_re()\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'list'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords if not already installed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the columns to process\n",
    "columns_to_process = ['headlines', 'description', 'content']\n",
    "\n",
    "# Get the set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    # Ensure the tokens are a list and filter out stopwords\n",
    "    return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Apply tokenization and stopword removal to both train and test datasets\n",
    "for col in columns_to_process:\n",
    "    # Tokenize the text (only once)\n",
    "    train_df[col] = train_df[col].apply(word_tokenize)\n",
    "    test_df[col] = test_df[col].apply(word_tokenize)\n",
    "    \n",
    "    # Remove stopwords (after tokenization)\n",
    "    train_df[col] = train_df[col].apply(remove_stopwords)\n",
    "    test_df[col] = test_df[col].apply(remove_stopwords)\n",
    "\n",
    "# Display the updated train and test DataFrames\n",
    "print(\"Train DataFrame (After Removing Stopwords):\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest DataFrame (After Removing Stopwords):\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f50a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
